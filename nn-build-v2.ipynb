{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of Z\n",
    "\n",
    "    Arguments:\n",
    "    Z -- A scalar or numpy array of any size.\n",
    "\n",
    "    Return:\n",
    "    A -- sigmoid of Z\n",
    "    backprop_store -- returns Z for backpropagation\n",
    "    \"\"\"\n",
    "\n",
    "    A = 1/(1 + np.exp(-Z))\n",
    "    backprop_store = Z\n",
    "\n",
    "    return A, backprop_store\n",
    "\n",
    "def leaky_relu(Z):\n",
    "    \"\"\"\n",
    "    Compute leaky_ReLU of Z\n",
    "\n",
    "    arguments:\n",
    "    Z -- A scalar of numpy array of any size\n",
    "\n",
    "    return:\n",
    "    A -- post-activation of leaky ReLU of Z, same shape as Z\n",
    "    backprop_store -- returns Z for backpropagation\n",
    "    \"\"\"\n",
    "\n",
    "    A = np.maximum(0.01 * Z, Z)\n",
    "    assert(A.shape == Z.shape)\n",
    "\n",
    "    backprop_store = Z\n",
    "\n",
    "    return A, backprop_store\n",
    "\n",
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Compute regular ReLU of Z\n",
    "\n",
    "    arguments:\n",
    "    Z -- A scalar of numpy array of any size\n",
    "\n",
    "    return:\n",
    "    A -- post-activation of ReLU of Z, same shape as Z\n",
    "    backprop_store -- returns Z for backpropagation\n",
    "    \"\"\"\n",
    "\n",
    "    A = np.maximum(0, Z)\n",
    "    assert(A.shape == Z.shape)\n",
    "\n",
    "    backprop_store = Z\n",
    "\n",
    "    return A, backprop_store\n",
    "\n",
    "\n",
    "def tanh(Z):\n",
    "    \"\"\"\n",
    "    Compute tanh of Z\n",
    "\n",
    "    arguments:\n",
    "    Z -- A scalar of numpy array of any size\n",
    "\n",
    "    return:\n",
    "    A -- post-activation of tanh of Z, same shape as Z\n",
    "    backprop_store -- returns Z for backpropagation\n",
    "    \"\"\"\n",
    "\n",
    "    A = (np.exp(Z)-np.exp(-Z))  /  (np.exp(Z)+np.exp(-Z))\n",
    "    assert(A.shape == Z.shape)\n",
    "\n",
    "    backprop_store = Z\n",
    "\n",
    "    return A, backprop_store\n",
    "\n",
    "\n",
    "def softmax(Z):\n",
    "    \"\"\"\n",
    "    Compute softmax of Z\n",
    "\n",
    "    arguments:\n",
    "    Z -- A scalar of numpy array of any size\n",
    "\n",
    "    return:\n",
    "    A -- post-activation of softmax of Z, same shape as Z\n",
    "    backprop_store -- returns Z for backpropagation\n",
    "    \"\"\"\n",
    "\n",
    "    e_Z = np.exp(Z - np.max(Z))\n",
    "    A = e_Z / e_Z.sum()\n",
    "    assert(A.shape == Z.shape)\n",
    "\n",
    "    backprop_store = Z\n",
    "\n",
    "    return A, backprop_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(initialize, dimension_of_layers):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    initialization -- activation used in this layer. \n",
    "        Stored as text string: \"He\", \"Xavier\", \"Yoshua\" \"random\"\n",
    "    dimensions_of_layers -- array (list) of size in each layer\n",
    "\n",
    "    Returns:\n",
    "    parameters -- dictionary containing parameters \"W1\", \"b1\", \"W2\", \"b2\",...\n",
    "                W[layer] -- shape (dimension_of_layers[layer], (dimension_of_layers[layer-1])\n",
    "                b[layer] -- bias vector shape (dimension_of_layers[layer], 1) \n",
    "    \"\"\"\n",
    "    \n",
    "    # np.random.seed(1)  # Use when you need to test that the different initializations are giving different numbers\n",
    "    parameters = {}\n",
    "    num_layers = len(dimension_of_layers)\n",
    "\n",
    "    for layer in range(1, num_layers):  # this will loop through first hidden layer to final output layer\n",
    "\n",
    "        if initialize == \"He\":\n",
    "            parameters[\"W\" + str(layer)] = np.random.randn(dimension_of_layers[layer], \n",
    "                dimension_of_layers[layer - 1]) * np.sqrt(2. / dimension_of_layers[layer - 1])\n",
    "            parameters[\"b\" + str(layer)] = np.zeros( (dimension_of_layers[layer], 1) )\n",
    "\n",
    "        elif initialize == \"Yoshua\":\n",
    "            parameters[\"W\" + str(layer)] = np.random.randn(dimension_of_layers[layer], \n",
    "                dimension_of_layers[layer - 1]) * np.sqrt(2. / (dimension_of_layers[layer - 1] + dimension_of_layers[layer]))\n",
    "            parameters[\"b\" + str(layer)] = np.zeros( (dimension_of_layers[layer], 1) )\n",
    "\n",
    "        elif initialize == \"Xavier\":\n",
    "            parameters[\"W\" + str(layer)] = np.random.randn(dimension_of_layers[layer], \n",
    "                dimension_of_layers[layer - 1]) * np.sqrt(1. / (dimension_of_layers[layer - 1]))\n",
    "            parameters[\"b\" + str(layer)] = np.zeros( (dimension_of_layers[layer], 1) )\n",
    "\n",
    "        elif initialize == \"random\":\n",
    "            parameters[\"W\" + str(layer)] = np.random.randn(dimension_of_layers[layer], dimension_of_layers[layer - 1]) * 0.01\n",
    "            parameters[\"b\" + str(layer)] = np.zeros( (dimension_of_layers[layer], 1) )\n",
    "\n",
    "        else:\n",
    "            print(\"ERROR: YOU MUST CHOOSE AN INITIALIZATION TYPE: \\\"He\\\", \\\"Yoshua\\\", \\\"Xavier\\\", or \\\"random\\\"\")\n",
    "\n",
    "            assert parameters[\"W\" + str(layer)].shape == (dimension_of_layers[layer], dimension_of_layers[layer - 1])\n",
    "            assert parameters[\"b\" + str(layer)].shape == (dimension_of_layers[layer], 1)\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_forward(A_prev, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    A_post -- the input of the activation function, also called pre-activation parameter \n",
    "    backprop_store -- a python dictionary containing \"W\", \"b\", and \"A_prev\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "\n",
    "    Z = np.dot(W, A_prev) + b\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    backprop_store = (W, b, A_prev)\n",
    "    \n",
    "    return Z, backprop_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation_type):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation_type -- the activation to be used in this layer, stored as a text string: \"sigmoid\", \"relu\",\n",
    "                         \"leaky relu\", \"tanh\", \"softmax\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python dictionary containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "             \"linear_cache\" and \"activation_cache\" are caching, storing, exactly what's being passed in it's function.\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation_type == \"sigmoid\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    \n",
    "    elif activation_type == \"relu\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "\n",
    "    elif activation_type == \"leaky relu\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = leaky_relu(Z)\n",
    "\n",
    "    elif activation_type == \"tanh\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = tanh(Z)\n",
    "\n",
    "    elif activation_type == \"softmax\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = softmax(Z)\n",
    "    \n",
    "    else:\n",
    "        print(\"ERROR: YOU MUST CHOOSE AN ACTIVATION TYPE: \\\"sigmoid\\\", \\\"relu\\\", \\\"leaky relu\\\", \\\"tanh\\\", or \\\"softmax\\\"\")\n",
    "\n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: L_model_forward\n",
    "\n",
    "def L_model_forward(X, parameters, dimension_of_layers, activation_type):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "    dimension_of_layers -- array (list) of size in each layer\n",
    "    \n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() (there are L-1 of them, indexed from 0 to L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(dimension_of_layers)\n",
    "    \n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for layer in range(1, L-1):    # This will loop through first hidden layer to last hidden layer (before output layer L)\n",
    "        A_prev = A \n",
    "        A, cache = linear_activation_forward(A_prev, parameters[\"W\" + str(l)], parameters[\"b\" + str(l)], \"relu\")\n",
    "        caches.append(cache)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    AL, cache = linear_activation_forward(A, parameters[\"W\" + str(L)], parameters[\"b\" + str(L)], \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "            \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dimension_of_layers = [3,6,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "X = np.random.randn(3,10)\n",
    "W = np.random.randn(6,3)\n",
    "b = np.zeros( (6,1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.27943278,  0.61155971,  0.46846374,  0.63395108,  0.15876591,\n",
       "          0.92400914,  0.25846549,  0.82021806,  0.48667068,  0.39575125],\n",
       "        [ 0.08783176,  0.8682329 ,  0.65732966,  0.76102554,  0.18184536,\n",
       "          0.93656662,  0.20465886,  0.776344  ,  0.42687888,  0.45334612],\n",
       "        [ 0.03560665,  0.8909622 ,  0.88194867,  0.87466511,  0.68870824,\n",
       "          0.76474065,  0.10030055,  0.2873127 ,  0.31190863,  0.78508178],\n",
       "        [ 0.87009434,  0.25448526,  0.24409583,  0.23711838,  0.40734488,\n",
       "          0.29112047,  0.80797199,  0.60686931,  0.61453802,  0.3169969 ],\n",
       "        [ 0.76941097,  0.04873223,  0.47371406,  0.54418778,  0.78885331,\n",
       "          0.45595422,  0.16774118,  0.27597057,  0.45498753,  0.76846316],\n",
       "        [ 0.04448311,  0.91687192,  0.89738116,  0.8409777 ,  0.82595416,\n",
       "          0.45514117,  0.19742643,  0.16137053,  0.31912398,  0.79963234]]),\n",
       " ((array([[-0.69166075, -0.39675353, -0.6871727 ],\n",
       "          [-0.84520564, -0.67124613, -0.0126646 ],\n",
       "          [-1.11731035,  0.2344157 ,  1.65980218],\n",
       "          [ 0.74204416, -0.19183555, -0.88762896],\n",
       "          [-0.74715829,  1.6924546 ,  0.05080775],\n",
       "          [-0.63699565,  0.19091548,  2.10025514]]), array([[ 0.],\n",
       "          [ 0.],\n",
       "          [ 0.],\n",
       "          [ 0.],\n",
       "          [ 0.],\n",
       "          [ 0.]]), array([[ 1.62434536, -0.61175641, -0.52817175, -1.07296862,  0.86540763,\n",
       "           -2.3015387 ,  1.74481176, -0.7612069 ,  0.3190391 , -0.24937038],\n",
       "          [ 1.46210794, -2.06014071, -0.3224172 , -0.38405435,  1.13376944,\n",
       "           -1.09989127, -0.17242821, -0.87785842,  0.04221375,  0.58281521],\n",
       "          [-1.10061918,  1.14472371,  0.90159072,  0.50249434,  0.90085595,\n",
       "           -0.68372786, -0.12289023, -0.93576943, -0.26788808,  0.53035547]])),\n",
       "  array([[-0.94727696,  0.45387311, -0.1263127 ,  0.54920481, -1.66743913,\n",
       "           2.49810884, -1.05395951,  1.51782557, -0.0533299 , -0.42320009],\n",
       "         [-2.34040126,  1.88542399,  0.65141676,  1.15831024, -1.50389474,\n",
       "           2.69223039, -1.35742662,  1.24448658, -0.29459676, -0.18715995],\n",
       "         [-3.29896694,  2.10060736,  2.01101435,  1.94285177,  0.79408712,\n",
       "           1.17884824, -2.19388958, -0.90847159, -0.79121134,  1.29553029],\n",
       "         [ 1.90179317, -1.07483196, -1.13035372, -1.16854335, -0.37495244,\n",
       "          -0.88994845,  1.43688616,  0.43417137,  0.46642822, -0.76760738],\n",
       "         [ 1.20498821, -2.9714549 , -0.10524077,  0.17721345,  1.31802729,\n",
       "          -0.17664099, -1.60172127, -0.96453781, -0.18053865,  1.1996536 ],\n",
       "         [-3.06714296,  2.40058526,  2.16845921,  1.66552073,  1.55722058,\n",
       "          -0.17991909, -1.40245754, -1.64806584, -0.75780057,  1.38399809]])))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_activation_forward(X, W, b, \"sigmoid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: YOU MUST CHOOSE AN INITIALIZATION TYPE: \"He\", \"Yoshua\", \"Xavier\", or \"random\"\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'W1'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-73-0f1003d5eb68>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mparameters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minitialize_parameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"e\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdimension_of_layers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-72-8aad01a457c3>\u001b[0m in \u001b[0;36minitialize_parameters\u001b[1;34m(initialize, dimension_of_layers)\u001b[0m\n\u001b[0;32m     40\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ERROR: YOU MUST CHOOSE AN INITIALIZATION TYPE: \\\"He\\\", \\\"Yoshua\\\", \\\"Xavier\\\", or \\\"random\\\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m             \u001b[1;32massert\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"W\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdimension_of_layers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdimension_of_layers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlayer\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"b\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdimension_of_layers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'W1'"
     ]
    }
   ],
   "source": [
    "parameters = initialize_parameters(\"e\", dimension_of_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[-0.48076113, -0.64897312, -0.72554173],\n",
       "        [ 0.18495424, -0.32140178, -1.18464144],\n",
       "        [ 0.4291821 ,  0.78888323, -0.31393061],\n",
       "        [ 0.34485952,  1.33616299,  1.50199522],\n",
       "        [ 0.50671875, -1.03611296, -0.7281231 ],\n",
       "        [-0.04424519, -0.25345354, -0.29557688]]),\n",
       " 'W2': array([[ 0.94738169, -0.00459634,  1.24610501, -0.11114517,  0.75611052,\n",
       "          0.34187951]]),\n",
       " 'b1': array([[ 0.],\n",
       "        [ 0.],\n",
       "        [ 0.],\n",
       "        [ 0.],\n",
       "        [ 0.],\n",
       "        [ 0.]]),\n",
       " 'b2': array([[ 0.]])}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
