{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.random.seed(1) just so you remember how to set random seeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ctrl + ' = hashtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of Z\n",
    "\n",
    "    Arguments:\n",
    "    Z -- A scalar or numpy array of any size.\n",
    "\n",
    "    Return:\n",
    "    A -- sigmoid of Z\n",
    "    backprop_store -- returns Z for backpropagation\n",
    "    \"\"\"\n",
    "\n",
    "    A = 1 / (1+np.exp(-Z))\n",
    "    backprop_store = Z\n",
    "\n",
    "    return A, backprop_store\n",
    "\n",
    "def leaky_relu(Z):\n",
    "    \"\"\"\n",
    "    Compute leaky_ReLU of Z\n",
    "\n",
    "    arguments:\n",
    "    Z -- A scalar of numpy array of any size\n",
    "\n",
    "    return:\n",
    "    A -- post-activation of leaky ReLU of Z, same shape as Z\n",
    "    backprop_store -- returns Z for backpropagation\n",
    "    \"\"\"\n",
    "\n",
    "    A = np.maximum(0.01 * Z, Z)\n",
    "    assert(A.shape == Z.shape)\n",
    "\n",
    "    backprop_store = Z\n",
    "\n",
    "    return A, backprop_store\n",
    "\n",
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Compute regular ReLU of Z\n",
    "\n",
    "    arguments:\n",
    "    Z -- A scalar of numpy array of any size\n",
    "\n",
    "    return:\n",
    "    A -- post-activation of ReLU of Z, same shape as Z\n",
    "    backprop_store -- returns Z for backpropagation\n",
    "    \"\"\"\n",
    "\n",
    "    A = np.maximum(0, Z)\n",
    "    assert(A.shape == Z.shape)\n",
    "\n",
    "    backprop_store = Z\n",
    "\n",
    "    return A, backprop_store\n",
    "\n",
    "\n",
    "def tanh(Z):\n",
    "    \"\"\"\n",
    "    Compute tanh of Z\n",
    "\n",
    "    arguments:\n",
    "    Z -- A scalar of numpy array of any size\n",
    "\n",
    "    return:\n",
    "    A -- post-activation of tanh of Z, same shape as Z\n",
    "    backprop_store -- returns Z for backpropagation\n",
    "    \"\"\"\n",
    "\n",
    "    A = (np.exp(Z)-np.exp(-Z))  /  (np.exp(Z)+np.exp(-Z))\n",
    "    assert(A.shape == Z.shape)\n",
    "\n",
    "    backprop_store = Z\n",
    "\n",
    "    return A, backprop_store\n",
    "\n",
    "\n",
    "def softmax(Z):\n",
    "    \"\"\"\n",
    "    Compute softmax of Z\n",
    "\n",
    "    arguments:\n",
    "    Z -- A scalar of numpy array of any size\n",
    "\n",
    "    return:\n",
    "    A -- post-activation of softmax of Z, same shape as Z\n",
    "    backprop_store -- returns Z for backpropagation\n",
    "    \"\"\"\n",
    "\n",
    "    e_Z = np.exp(Z - np.max(Z))\n",
    "    A = e_Z / e_Z.sum()\n",
    "    assert(A.shape == Z.shape)\n",
    "\n",
    "    backprop_store = Z\n",
    "\n",
    "    return A, backprop_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "relu: try He initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tanh: try Xavier or even Yoshua"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_parameters_he(dimension_of_layers):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    dimensions_of_layers -- array (list) of size in each layer; including input layer\n",
    "\n",
    "    Returns:\n",
    "    parameters -- dictionary containing parameters \"W1\", \"b1\", \"W2\", \"b2\",...\n",
    "                W[layer] -- shape (dimension_of_layers[layer], (dimension_of_layers[layer-1])\n",
    "                b[layer] -- bias vector shape (dimension_of_layers[layer], 1) \n",
    "    \"\"\"\n",
    "    \n",
    "    parameters = {}\n",
    "    L = len(dimension_of_layers)  # number of layers\n",
    "    \n",
    "    for layer in range(1, L):\n",
    "        parameters[\"W\" + str(layer)] = np.random.randn(dimension_of_layers[layer], \n",
    "                dimension_of_layers[layer - 1]) * np.sqrt(2. / dimension_of_layers[layer - 1])\n",
    "        parameters[\"b\" + str(layer)] = np.zeros( (dimension_of_layers[layer], 1) )\n",
    "        \n",
    "    return parameters\n",
    "\n",
    "######################\n",
    "\n",
    "def initialize_parameters_yoshua(dimension_of_layers):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    dimensions_of_layers -- array (list) of size in each layer; including input layer\n",
    "\n",
    "    Returns:\n",
    "    parameters -- dictionary containing parameters \"W1\", \"b1\", \"W2\", \"b2\",...\n",
    "                W[layer] -- shape (dimension_of_layers[layer], (dimension_of_layers[layer-1])\n",
    "                b[layer] -- bias vector shape (dimension_of_layers[layer], 1) \n",
    "    \"\"\"\n",
    "    \n",
    "    parameters = {}\n",
    "    L = len(dimension_of_layers)  # number of layers\n",
    "    \n",
    "    \n",
    "    for layer in range(1, L):\n",
    "        parameters[\"W\" + str(layer)] = np.random.randn(dimension_of_layers[layer], \n",
    "                dimension_of_layers[layer - 1]) * np.sqrt(2. / (dimension_of_layers[layer - 1] + dimension_of_layers[layer]))\n",
    "        parameters[\"b\" + str(layer)] = np.zeros( (dimension_of_layers[layer], 1) )\n",
    "    \n",
    "    \n",
    "    return parameters\n",
    "\n",
    "#########################\n",
    "\n",
    "def initialize_parameters_xavier(dimension_of_layers):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    dimensions_of_layers -- array (list) of size in each layer; including input layer\n",
    "\n",
    "    Returns:\n",
    "    parameters -- dictionary containing parameters \"W1\", \"b1\", \"W2\", \"b2\",...\n",
    "                W[layer] -- shape (dimension_of_layers[layer], (dimension_of_layers[layer-1])\n",
    "                b[layer] -- bias vector shape (dimension_of_layers[layer], 1) \n",
    "    \"\"\"\n",
    "    \n",
    "    parameters = {}\n",
    "    L = len(dimension_of_layers)  # number of layers\n",
    "    \n",
    "    \n",
    "    for layer in range(1, L):\n",
    "        parameters[\"W\" + str(layer)] = np.random.randn(dimension_of_layers[layer], \n",
    "                dimension_of_layers[layer - 1]) * np.sqrt(1. / (dimension_of_layers[layer - 1]))\n",
    "        parameters[\"b\" + str(layer)] = np.zeros( (dimension_of_layers[layer], 1) )\n",
    "\n",
    "    return parameters\n",
    "\n",
    "################################\n",
    "\n",
    "def initialize_parameters_random(dimension_of_layers):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    dimensions_of_layers -- array (list) of size in each layer; including input layer\n",
    "\n",
    "    Returns:\n",
    "    parameters -- dictionary containing parameters \"W1\", \"b1\", \"W2\", \"b2\",...\n",
    "                W[layer] -- shape (dimension_of_layers[layer], (dimension_of_layers[layer-1])\n",
    "                b[layer] -- bias vector shape (dimension_of_layers[layer], 1) \n",
    "    \"\"\"\n",
    "    \n",
    "    parameters = {}\n",
    "    L = len(dimension_of_layers)  # number of layers\n",
    "    \n",
    "    \n",
    "    for layer in range(1, L):\n",
    "        parameters[\"W\" + str(layer)] = np.random.randn(dimension_of_layers[layer], dimension_of_layers[layer - 1]) * 0.01\n",
    "        parameters[\"b\" + str(layer)] = np.zeros( (dimension_of_layers[layer], 1) )\n",
    "\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(dimension_of_layers, initialization):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    dimensions_of_layers -- array (list) of size in each layer; including input layer\n",
    "    initialization -- stored as text string: \"He\", \"Yoshua\", \"Xavier\", or \"random\"\n",
    "\n",
    "    Returns:\n",
    "    parameters -- dictionary containing parameters \"W1\", \"b1\", \"W2\", \"b2\",...\n",
    "                W[layer] -- shape (dimension_of_layers[layer], (dimension_of_layers[layer-1])\n",
    "                b[layer] -- bias vector shape (dimension_of_layers[layer], 1) \n",
    "    \"\"\"\n",
    "    \n",
    "    if initialization == \"He\":\n",
    "        parameters = initialize_parameters_he(dimension_of_layers)\n",
    "    elif initialization == \"Yoshua\":\n",
    "        parameters = initialize_parameters_yoshua(dimension_of_layers)\n",
    "    elif initialization == \"Xavier\":\n",
    "        parameters = initialize_parameters_xavier(dimension_of_layers)\n",
    "    elif initialization == \"random\":\n",
    "        parameters = initialize_parameters_random(dimension_of_layers)\n",
    "    else:\n",
    "        print(\"ERROR: YOU MUST CHOOSE AN INITILIZATION: \\\"He\\\", \\\"Yoshua\\\", \\\"Xavier\\\", or \\\"random\\\"\")\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "try to make a dictionary of backrop store instead of a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_forward(A_prev, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter \n",
    "    backprop_store -- a python dictionary containing \"W\", \"b\", and \"A_prev\"; \n",
    "        stored in a backword order for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "\n",
    "    Z = np.dot(W, A_prev) + b\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    backprop_store = (W, b, A_prev)\n",
    "    \n",
    "    return Z, backprop_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR BACKPROP CACHE IS ORDERED AS W,B, A_PREV, Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_activation_forward_sigmoid(A_prev, W, b):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    \n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python dictionary containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "             \"linear_cache\" and \"activation_cache\" are caching, storing, exactly what's being passed in it's function.\n",
    "             W, b, A_prev, Z (this order)\n",
    "    \"\"\"         \n",
    "    \n",
    "    Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "    A, activation_cache = sigmoid(Z)\n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache\n",
    "\n",
    "###############################\n",
    "\n",
    "def linear_activation_forward_relu(A_prev, W, b):\n",
    "    \n",
    "    Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "    A, activation_cache = relu(Z)\n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache\n",
    "\n",
    "################################\n",
    "\n",
    "def linear_activation_forward_leaky_relu(A_prev, W, b):\n",
    "    \n",
    "    Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "    A, activation_cache = leaky_relu(Z)\n",
    "        \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache        \n",
    "\n",
    "###################################\n",
    "\n",
    "def linear_activation_forward_tanh(A_prev, W, b):\n",
    "    \n",
    "    Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "    A, activation_cache = tanh(Z)\n",
    "        \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache \n",
    "\n",
    "######################################\n",
    "\n",
    "def linear_activation_forward_softmax(A_prev, W, b):\n",
    "    \n",
    "    Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "    A, activation_cache = softmax(Z)\n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters, dimension_of_layers, hidden_activation, output_activation):  \n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(dimension_of_layers)\n",
    "    \"\"\"\n",
    "    arguments:\n",
    "    X -- data input. shape: (features, number of examples)\n",
    "    parameters -- dictionary containing parameters \"W1\", \"b1\", \"W2\", \"b2\",...\n",
    "                W[layer] -- shape (dimension_of_layers[layer], (dimension_of_layers[layer-1])\n",
    "                b[layer] -- bias vector shape (dimension_of_layers[layer], 1)\n",
    "    dimension_of_layers -- array (list) of size in each layer; including input layer\n",
    "    hidden_activation -- stored as text string: \"relu\", \"sigmoid\", \"relu\", \"leaky relu\", or \"tanh\"\n",
    "    output activation -- stored as text string: \"sigmoid\" of \"softmax\"\n",
    "    \n",
    "    Returns:\n",
    "    AL -- activation of ouput layer\n",
    "    caches -- a python dictionary containing \"linear_cache\" and \"activation_cache\" for all hidden layers and output layer;\n",
    "             stored for computing the backward pass efficiently\n",
    "             \"linear_cache\" and \"activation_cache\" are caching, storing, exactly what's being passed in it's function.\n",
    "             W, b, A_prev, Z (this order)\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # when using range in the loop below it will end at the final hidden loop\n",
    "    # Calculating AL for the output layer also used L-1 because you did not use\n",
    "    # a loop with range. So, you need to put L-1 due to Python starting index from 0.\n",
    "    \n",
    "    # Implement [LINEAR -> hidden_activation]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    \n",
    "    for layer in range(1, L-1):    # This will loop through first hidden layer to last hidden layer (before output layer L)\n",
    "        \n",
    "        A_prev = A \n",
    "        \n",
    "        if hidden_activation == 'sigmoid':\n",
    "            A, cache = linear_activation_forward_sigmoid(A_prev, parameters['W' + str(layer)], parameters['b' + str(layer)])\n",
    "            caches.append(cache)\n",
    "        elif hidden_activation == 'relu':\n",
    "            A, cache = linear_activation_forward_relu(A_prev, parameters['W' + str(layer)], parameters['b' + str(layer)])\n",
    "            caches.append(cache)\n",
    "        elif hidden_activation == 'leaky relu':\n",
    "            A, cache = linear_activation_forward_leaky_relu(A_prev, parameters['W' + str(layer)], parameters['b' + str(layer)])\n",
    "            caches.append(cache)\n",
    "        elif hidden_activation == 'tanh':\n",
    "            A, cache = linear_activation_forward_tanh(A_prev, parameters['W' + str(layer)], parameters['b' + str(layer)])\n",
    "            caches.append(cache)\n",
    "        else:\n",
    "            print(\"ERROR: YOU MUST CHOOSE A HIDDEN ACTIVATION. HIDDEN TYPES: \\\"sigmoid\\\", \\\"relu\\\", \\\"leaky relu\\\", or \\\"tanh\\\"\")\n",
    "            return\n",
    "    \n",
    "    if output_activation == 'sigmoid':\n",
    "        AL, cache = linear_activation_forward_sigmoid(A, parameters[\"W\" + str(L-1)], parameters[\"b\" + str(L-1)])\n",
    "        caches.append(cache)\n",
    "    elif output_activation == 'softmax':\n",
    "        AL, cache = linear_activation_forward_softmax(A, parameters[\"W\" + str(L-1)], parameters[\"b\" + str(L-1)])\n",
    "        caches.append(cache)\n",
    "    else:\n",
    "        print(\"ERROR: YOU MUST CHOOSE AN OUTPUT ACTIVATION. OUTPUT TYPES: \\\"sigmoid\\\" or \\\"softmax\\\"\")\n",
    "            \n",
    "        \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the cross-entropy for logistic regression (sigmoid). You still need to do the softmax cost which needs to be turned into a one hot before it can be done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cost_sigmoid(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function, cross-entropy cost for \"sigmoid\" function\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector, shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # Compute loss from aL and y.\n",
    "    cost = cost = (   (    np.dot(Y, np.log(AL.T))   )  +  (np.dot(  (1 - Y), np.log(1 - AL.T)  )  )   )  /  -m\n",
    "    \n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost\n",
    "\n",
    "#def compute_cost_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y, output_activation):\n",
    "    \"\"\"\n",
    "    Implement the cost function, cross-entropy cost for \"sigmoid\" function\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector, shape (1, number of examples)\n",
    "    output_activation -- stored as text string: \"sigmoid\" of \"softmax\"\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    if output_activation == \"sigmoid\":\n",
    "        cost = cost_sigmoid(AL, Y)\n",
    "    else:\n",
    "        print(\"ERROR: YOU MUST CHOOSE AN OUTPUT ACTIVATION. OUTPUT TYPES: \\\"sigmoid\\\" or \\\"softmax\\\"\")\n",
    "        return\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### need to also make backward function softmax. Or do you???????\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single RELU unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    \n",
    "    # When z <= 0, you should set dz to 0 as well. \n",
    "    dZ[Z <= 0] = 0  # wherever Z is less than or equal to 0 then 0 \n",
    "                    # will be placed in dZ for that index.This works because they are\n",
    "                    # the same size\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "###################################\n",
    "\n",
    "def sigmoid_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single SIGMOID unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    \n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "######################################\n",
    "\n",
    "def leaky_relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single RELU unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    \n",
    "    # When z <= 0, you should set dz to 0 as well. \n",
    "    dZ[Z <= 0] = 0.01  # wherever Z is less than or equal to 0 then 0.01 \n",
    "                    # will be placed in dZ for that index.This works because they are\n",
    "                    # the same size\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "\n",
    "#########################################\n",
    "\n",
    "def tanh_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single tanh unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    dZ = dA * (1  -  ((np.exp(Z)-np.exp(-Z))  /  (np.exp(Z)+np.exp(-Z)))**2)  # you can't use your tanh function because it also \n",
    "                                                                              # returns a backprop store\n",
    "    \n",
    "    assert(dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    W, b, A_prev = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = np.dot(dZ, A_prev.T) / m\n",
    "    db = np.sum(dZ, axis = 1, keepdims = True) / m       ## remember this one for that type of formula!\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_activation_backward_sigmoid(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    dZ = sigmoid_backward(dA, activation_cache)\n",
    "    dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "\n",
    "########################################\n",
    "\n",
    "def linear_activation_backward_relu(dA, cache):\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    dZ = relu_backward(dA, activation_cache)\n",
    "    dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db \n",
    "\n",
    "######################################\n",
    "def linear_activation_backward_leaky_relu(dA, cache):\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    dZ = leaky_relu_backward(dA, activation_cache)\n",
    "    dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "\n",
    "#########################################\n",
    "def linear_activation_backward_tanh(dA, cache):\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    dZ = tanh_backward(dA, activation_cache)\n",
    "    dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches, hidden_activation, output_activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "    hidden_activation -- stored as text string: \"relu\", \"sigmoid\", \"relu\", \"leaky relu\", or \"tanh\"\n",
    "    output activation -- stored as text string: \"sigmoid\" of \"softmax\"\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers NOT including input layer                    \n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    if output_activation == \"sigmoid\":\n",
    "        dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    else:\n",
    "        print(\"ERROR: YOU MUST CHOOSE AN OUTPUT ACTIVATION. OUTPUT TYPES: \\\"sigmoid\\\" or \\\"softmax\\\"\")\n",
    "        ############ also need to make a derivate of cost function for softmax\n",
    "        \n",
    "    \n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"dAL, current_cache\". Outputs: \"grads[\"dAL-1\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    current_cache = caches[L - 1]    # this is accounting that Python indexes from 0\n",
    "    \n",
    "    if output_activation == \"sigmoid\":\n",
    "        grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward_sigmoid(dAL, current_cache)\n",
    "    else:\n",
    "        print(\"ERROR: YOU MUST CHOOSE AN OUTPUT ACTIVATION. OUTPUT TYPES: \\\"sigmoid\\\" or \\\"softmax\\\"\")\n",
    "        ##########also need to add softmax here\n",
    "    \n",
    "    \n",
    "    # Loop from l=L-2 to l=0\n",
    "    for l in reversed(range(L-1)): # This is actually accounting Python indexes from 0 AND that range goes only to number before.\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        # Inputs: \"grads[\"dA\" + str(l + 1)], current_cache\". Outputs: \"grads[\"dA\" + str(l)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
    "        current_cache = caches[l]\n",
    "        \n",
    "        if hidden_activation == \"sigmoid\":\n",
    "            grads[\"dA\" + str(l)], grads[\"dW\" + str(l+1)], grads[\"db\" + str(l+1)] = linear_activation_backward_sigmoid(grads[\"dA\" + str(l+1)], current_cache)\n",
    "        \n",
    "        elif hidden_activation == \"relu\":\n",
    "            grads[\"dA\" + str(l)], grads[\"dW\" + str(l+1)], grads[\"db\" + str(l+1)] = linear_activation_backward_relu(grads[\"dA\" + str(l+1)], current_cache)\n",
    "        \n",
    "        elif hidden_activation == 'leaky relu':\n",
    "            grads[\"dA\" + str(l)], grads[\"dW\" + str(l+1)], grads[\"db\" + str(l+1)] = linear_activation_backward_leaky_relu(grads[\"dA\" + str(l+1)], current_cache)\n",
    "        \n",
    "        elif hidden_activation == 'tanh':\n",
    "            grads[\"dA\" + str(l)], grads[\"dW\" + str(l+1)], grads[\"db\" + str(l+1)] = linear_activation_backward_tanh(grads[\"dA\" + str(l+1)], current_cache)\n",
    "        \n",
    "        else:\n",
    "            print(\"ERROR: YOU MUST CHOOSE A HIDDEN ACTIVATION. HIDDEN TYPES: \\\"sigmoid\\\", \\\"relu\\\", \\\"leaky relu\\\", or \\\"tanh\\\"\")\n",
    "            break\n",
    "            \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_parameters_gradient_descent(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    learning_rate -- the learning rate, scalar.\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network not including input layer\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - (learning_rate * grads[\"dW\" + str(l + 1)])\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - (learning_rate * grads[\"db\" + str(l + 1)])\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, update_type, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    update_type -- stored as text string: \"gradient descent\" or ect\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    if update_type == \"gradient descent\":\n",
    "        parameters = update_parameters_gradient_descent(parameters, grads, learning_rate)\n",
    "    else:\n",
    "        print(\"choose update type such as adam, momentum, etc\")\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, dimension_of_layers, initialization, hidden_activation, output_activation, update_type,\n",
    "                  learning_rate=.01, num_iterations=100, print_cost=False):\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (number of examples, num_px * num_px * 3)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    dimension_of_layers -- list containing the input size and each layer size; including input layer\n",
    "    initialization -- stored as text string: \"He\", \"Yoshua\", \"Xavier\", or \"random\"\n",
    "    hidden_activation -- stored as text string: \"relu\", \"sigmoid\", \"relu\", \"leaky relu\", or \"tanh\"\n",
    "    output activation -- stored as text string: \"sigmoid\" of \"softmax\"\n",
    "    update_type -- stored as text string: \"gradient descent\" or ect\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    costs = []\n",
    "    \n",
    "    parameters = initialize_parameters(dimension_of_layers, initialization)\n",
    "    \n",
    "    for i in range(1, num_iterations):\n",
    "        \n",
    "        AL, caches = L_model_forward(X, parameters, dimension_of_layers, hidden_activation, output_activation)\n",
    "        \n",
    "        cost = compute_cost(AL, Y, output_activation) ############################### need to clean this for soft max cost\n",
    "        \n",
    "        grads = L_model_backward(AL, Y, caches, hidden_activation, output_activation)\n",
    "        \n",
    "        parameters = update_parameters(parameters, grads, update_type, learning_rate) ################## maybe add adam\n",
    "        \n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()   \n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 100: 0.604461\n",
      "Cost after iteration 200: 0.578247\n",
      "Cost after iteration 300: 0.550972\n",
      "Cost after iteration 400: 0.521209\n",
      "Cost after iteration 500: 0.486577\n",
      "Cost after iteration 600: 0.430437\n",
      "Cost after iteration 700: 0.403134\n",
      "Cost after iteration 800: 0.377130\n",
      "Cost after iteration 900: 0.351309\n",
      "Cost after iteration 1000: 0.325694\n",
      "Cost after iteration 1100: 0.300663\n",
      "Cost after iteration 1200: 0.273840\n",
      "Cost after iteration 1300: 0.245496\n",
      "Cost after iteration 1400: 0.215843\n",
      "Cost after iteration 1500: 0.186821\n",
      "Cost after iteration 1600: 0.163960\n",
      "Cost after iteration 1700: 0.148582\n",
      "Cost after iteration 1800: 0.134406\n",
      "Cost after iteration 1900: 0.125990\n",
      "Cost after iteration 2000: 0.112297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\P\\Anaconda3\\envs\\py3\\lib\\site-packages\\ipykernel_launcher.py:16: RuntimeWarning: divide by zero encountered in log\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\P\\Anaconda3\\envs\\py3\\lib\\site-packages\\ipykernel_launcher.py:27: RuntimeWarning: invalid value encountered in true_divide\n",
      "C:\\Users\\P\\Anaconda3\\envs\\py3\\lib\\site-packages\\ipykernel_launcher.py:49: RuntimeWarning: invalid value encountered in maximum\n",
      "C:\\Users\\P\\Anaconda3\\envs\\py3\\lib\\site-packages\\ipykernel_launcher.py:19: RuntimeWarning: invalid value encountered in less_equal\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 2100: 0.103019\n",
      "Cost after iteration 2200: nan\n",
      "Cost after iteration 2300: nan\n",
      "Cost after iteration 2400: nan\n",
      "Cost after iteration 2500: nan\n",
      "Cost after iteration 2600: nan\n",
      "Cost after iteration 2700: nan\n",
      "Cost after iteration 2800: nan\n",
      "Cost after iteration 2900: nan\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUBARkSz/D9MyVu0KrXKUAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x27d828ab710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "X = np.random.rand(2,8)*10\n",
    "dimension_of_layers = [2,5,3,8,1]\n",
    "Y = np.array([[1,1,0,0,1,1,0,1]])\n",
    "\n",
    "paremeters = L_layer_model(X, Y, dimension_of_layers, initialization=\"He\", hidden_activation=\"relu\", \n",
    "                           output_activation=\"sigmoid\", update_type=\"gradient descent\",\n",
    "                           learning_rate=.01, num_iterations=3000, print_cost=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
