{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dimension_of_layers = [3,6,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A_prev = np.array([ [2,2,2], [.2,.2,.2], [-.2, -.2, -.2], [-10, -10, -10] ])\n",
    "W = np.ones( (2,4) )\n",
    "b = np.zeros( (2,1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters = initialize_parameters(\"He\", dimension_of_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A, cache = linear_activation_forward(A_prev, W, b, \"sigmoid\")\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of Z\n",
    "\n",
    "    Arguments:\n",
    "    Z -- A scalar or numpy array of any size.\n",
    "\n",
    "    Return:\n",
    "    A -- sigmoid of Z\n",
    "    backprop_store -- returns Z for backpropagation\n",
    "    \"\"\"\n",
    "\n",
    "    A = 1/(1 + np.exp(-Z))\n",
    "    backprop_store = Z\n",
    "\n",
    "    return A, backprop_store\n",
    "\n",
    "def leaky_relu(Z):\n",
    "    \"\"\"\n",
    "    Compute leaky_ReLU of Z\n",
    "\n",
    "    arguments:\n",
    "    Z -- A scalar of numpy array of any size\n",
    "\n",
    "    return:\n",
    "    A -- post-activation of leaky ReLU of Z, same shape as Z\n",
    "    backprop_store -- returns Z for backpropagation\n",
    "    \"\"\"\n",
    "\n",
    "    A = np.maximum(0.01 * Z, Z)\n",
    "    assert(A.shape == Z.shape)\n",
    "\n",
    "    backprop_store = Z\n",
    "\n",
    "    return A, backprop_store\n",
    "\n",
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Compute regular ReLU of Z\n",
    "\n",
    "    arguments:\n",
    "    Z -- A scalar of numpy array of any size\n",
    "\n",
    "    return:\n",
    "    A -- post-activation of ReLU of Z, same shape as Z\n",
    "    backprop_store -- returns Z for backpropagation\n",
    "    \"\"\"\n",
    "\n",
    "    A = np.maximum(0, Z)\n",
    "    assert(A.shape == Z.shape)\n",
    "\n",
    "    backprop_store = Z\n",
    "\n",
    "    return A, backprop_store\n",
    "\n",
    "\n",
    "def tanh(Z):\n",
    "    \"\"\"\n",
    "    Compute tanh of Z\n",
    "\n",
    "    arguments:\n",
    "    Z -- A scalar of numpy array of any size\n",
    "\n",
    "    return:\n",
    "    A -- post-activation of tanh of Z, same shape as Z\n",
    "    backprop_store -- returns Z for backpropagation\n",
    "    \"\"\"\n",
    "\n",
    "    A = (np.exp(Z)-np.exp(-Z))  /  (np.exp(Z)+np.exp(-Z))\n",
    "    assert(A.shape == Z.shape)\n",
    "\n",
    "    backprop_store = Z\n",
    "\n",
    "    return A, backprop_store\n",
    "\n",
    "\n",
    "def softmax(Z):\n",
    "    \"\"\"\n",
    "    Compute softmax of Z\n",
    "\n",
    "    arguments:\n",
    "    Z -- A scalar of numpy array of any size\n",
    "\n",
    "    return:\n",
    "    A -- post-activation of softmax of Z, same shape as Z\n",
    "    backprop_store -- returns Z for backpropagation\n",
    "    \"\"\"\n",
    "\n",
    "    e_Z = np.exp(Z - np.max(Z))\n",
    "    A = e_Z / e_Z.sum()\n",
    "    assert(A.shape == Z.shape)\n",
    "\n",
    "    backprop_store = Z\n",
    "\n",
    "    return A, backprop_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_parameters(initialize, dimension_of_layers):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    initialization -- activation used in this layer. \n",
    "        Stored as text string: \"He\", \"Xavier\", \"Yoshua\" \"random\"\n",
    "    dimensions_of_layers -- array (list) of size in each layer\n",
    "\n",
    "    Returns:\n",
    "    parameters -- dictionary containing parameters \"W1\", \"b1\", \"W2\", \"b2\",...\n",
    "                W[layer] -- shape (dimension_of_layers[layer], (dimension_of_layers[layer-1])\n",
    "                b[layer] -- bias vector shape (dimension_of_layers[layer], 1) \n",
    "    \"\"\"\n",
    "    \n",
    "    # np.random.seed(1)  # Use when you need to test that the different initializations are giving different numbers\n",
    "    parameters = {}\n",
    "    num_layers = len(dimension_of_layers)\n",
    "\n",
    "    for layer in range(1, num_layers):  # this will loop through first hidden layer to final output layer\n",
    "\n",
    "        if initialize == \"He\":\n",
    "            parameters[\"W\" + str(layer)] = np.random.randn(dimension_of_layers[layer], \n",
    "                dimension_of_layers[layer - 1]) * np.sqrt(2. / dimension_of_layers[layer - 1])\n",
    "            parameters[\"b\" + str(layer)] = np.zeros( (dimension_of_layers[layer], 1) )\n",
    "\n",
    "        elif initialize == \"Yoshua\":\n",
    "            parameters[\"W\" + str(layer)] = np.random.randn(dimension_of_layers[layer], \n",
    "                dimension_of_layers[layer - 1]) * np.sqrt(2. / (dimension_of_layers[layer - 1] + dimension_of_layers[layer]))\n",
    "            parameters[\"b\" + str(layer)] = np.zeros( (dimension_of_layers[layer], 1) )\n",
    "\n",
    "        elif initialize == \"Xavier\":\n",
    "            parameters[\"W\" + str(layer)] = np.random.randn(dimension_of_layers[layer], \n",
    "                dimension_of_layers[layer - 1]) * np.sqrt(1. / (dimension_of_layers[layer - 1]))\n",
    "            parameters[\"b\" + str(layer)] = np.zeros( (dimension_of_layers[layer], 1) )\n",
    "\n",
    "        elif initialize == \"random\":\n",
    "            parameters[\"W\" + str(layer)] = np.random.randn(dimension_of_layers[layer], dimension_of_layers[layer - 1]) * 0.01\n",
    "            parameters[\"b\" + str(layer)] = np.zeros( (dimension_of_layers[layer], 1) )\n",
    "\n",
    "        else:\n",
    "            print(\"ERROR: YOU MUST CHOOSE AN INITIALIZATION TYPE\")\n",
    "\n",
    "            assert parameters[\"weights\" + str(layer)].shape == (dimension_of_layers[layer], dimension_of_layers[layer - 1])\n",
    "            assert parameters[\"bias\" + str(layer)].shape == (dimension_of_layers[layer], 1)\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_forward(A_prev, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    A_post -- the input of the activation function, also called pre-activation parameter \n",
    "    backprop_store -- a python dictionary containing \"W\", \"b\", and \"A_prev\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "\n",
    "    Z = np.dot(W, A_prev) + b\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    backprop_store = (W, b, A_prev)\n",
    "    \n",
    "    return Z, backprop_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation_type):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation_type -- the activation to be used in this layer, stored as a text string: \"sigmoid\", \"relu\",\n",
    "                         \"leaky relu\", \"tanh\", \"softmax\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python dictionary containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "             \"linear_cache\" and \"activation_cache\" are caching, storing, exactly what's being passed in it's function.\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation_type == \"sigmoid\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    \n",
    "    elif activation_type == \"relu\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "\n",
    "    elif activation_type == \"leaky relu\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = leaky_relu(Z)\n",
    "\n",
    "    elif activation_type == \"tanh\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = tanh(Z)\n",
    "\n",
    "    elif activation_type == \"softmax\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = softmax(Z)\n",
    "    \n",
    "    else:\n",
    "        print(\"ERROR: YOU MUST CHOOSE AN ACTIVATION TYPE\")\n",
    "\n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters = initialize_parameters(\"Xavier\", dimension_of_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(parameters)//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A, cache = linear_activation_forward(A_prev, W, b, \"leaky relu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.3196312 ,  0.99876919,  0.15412092],\n",
       "       [ 0.82931795, -1.22734271, -0.05869624],\n",
       "       [-0.49346818,  0.41789699, -0.68197933],\n",
       "       [ 0.29509439,  0.01623048,  0.15418425],\n",
       "       [ 0.25720141,  0.22962094, -0.15783222],\n",
       "       [ 0.88814178, -0.00657929,  0.52418399]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters[\"W1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "#input_layer, hidden, hidden, output\n",
    "#0,1,2,3\n",
    "for i in range(4):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: L_model_forward\n",
    "\n",
    "def L_model_forward(X, parameters, dimension_of_layers):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "    \n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() (there are L-1 of them, indexed from 0 to L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2        # number of layers in the neural network EXCLUDING THE INPUT LAYER\n",
    "    ##num_layers = len(dimension_of_layers) maybe have this instead\n",
    "    \n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, L):        ### this really prints 1, L-1.  Look at the cell above!!!!!\n",
    "        A_prev = A \n",
    "        A, cache = linear_activation_forward(A_prev, parameters[\"W\" + str(l)], parameters[\"b\" + str(l)], \"relu\") # updating A each iteration\n",
    "        caches.append(cache)\n",
    "    \n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    AL, cache = linear_activation_forward(A, parameters[\"W\" + str(L)], parameters[\"b\" + str(L)], \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "    \n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "            \n",
    "    return AL, caches"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
