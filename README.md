# neural-network-from-scratch
An unlimited layered neural network that I built from scratch.

not finished yet

need to:
1 one hot encode labels and do loss for softmax which is - sum Y*logYhat
2 use dZ of soft max to initialize backprop dZ = yhat-y
3. maybe add adam, momentum,mini batch gradient descent, batch and L2 regularization
4. move all functions to py files and import into jupyter notebook
