# neural-network-from-scratch | Python
An unlimited layered neural network that I built from scratch.

Programming language: Python

Still in progress and to be updated later...

Model works and can be run when hidden_activation is set to "leaky relu", "relu", "tanh", "sigmoid" and output_activation "sigmoid"


would like to add if I have more free time

1 one hot encode labels and do loss for softmax which is: - Î£ Y*log(Y-hat)

2 use dZ of soft max to initialize backprop dZ = yhat-y

3. maybe add adam, momentum,mini batch gradient descent, batch and L2 regularization

4. move all functions to py files and import into jupyter notebook

5. check your equations are right by using gradient checking algorithm
