{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of Z\n",
    "\n",
    "    Arguments:\n",
    "    Z -- A scalar or numpy array of any size.\n",
    "\n",
    "    Return:\n",
    "    A -- sigmoid of Z\n",
    "    backprop_store -- returns Z for backpropagation\n",
    "    \"\"\"\n",
    "\n",
    "    A = 1 / (1+np.exp(-Z))\n",
    "    backprop_store = Z\n",
    "\n",
    "    return A, backprop_store\n",
    "\n",
    "def leaky_relu(Z):\n",
    "    \"\"\"\n",
    "    Compute leaky_ReLU of Z\n",
    "\n",
    "    arguments:\n",
    "    Z -- A scalar of numpy array of any size\n",
    "\n",
    "    return:\n",
    "    A -- post-activation of leaky ReLU of Z, same shape as Z\n",
    "    backprop_store -- returns Z for backpropagation\n",
    "    \"\"\"\n",
    "\n",
    "    A = np.maximum(0.01 * Z, Z)\n",
    "    assert(A.shape == Z.shape)\n",
    "\n",
    "    backprop_store = Z\n",
    "\n",
    "    return A, backprop_store\n",
    "\n",
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Compute regular ReLU of Z\n",
    "\n",
    "    arguments:\n",
    "    Z -- A scalar of numpy array of any size\n",
    "\n",
    "    return:\n",
    "    A -- post-activation of ReLU of Z, same shape as Z\n",
    "    backprop_store -- returns Z for backpropagation\n",
    "    \"\"\"\n",
    "\n",
    "    A = np.maximum(0, Z)\n",
    "    assert(A.shape == Z.shape)\n",
    "\n",
    "    backprop_store = Z\n",
    "\n",
    "    return A, backprop_store\n",
    "\n",
    "\n",
    "def tanh(Z):\n",
    "    \"\"\"\n",
    "    Compute tanh of Z\n",
    "\n",
    "    arguments:\n",
    "    Z -- A scalar of numpy array of any size\n",
    "\n",
    "    return:\n",
    "    A -- post-activation of tanh of Z, same shape as Z\n",
    "    backprop_store -- returns Z for backpropagation\n",
    "    \"\"\"\n",
    "\n",
    "    A = (np.exp(Z)-np.exp(-Z))  /  (np.exp(Z)+np.exp(-Z))\n",
    "    assert(A.shape == Z.shape)\n",
    "\n",
    "    backprop_store = Z\n",
    "\n",
    "    return A, backprop_store\n",
    "\n",
    "\n",
    "def softmax(Z):\n",
    "    \"\"\"\n",
    "    Compute softmax of Z\n",
    "\n",
    "    arguments:\n",
    "    Z -- A scalar of numpy array of any size\n",
    "\n",
    "    return:\n",
    "    A -- post-activation of softmax of Z, same shape as Z\n",
    "    backprop_store -- returns Z for backpropagation\n",
    "    \"\"\"\n",
    "\n",
    "    e_Z = np.exp(Z - np.max(Z))\n",
    "    A = e_Z / e_Z.sum()\n",
    "    assert(A.shape == Z.shape)\n",
    "\n",
    "    backprop_store = Z\n",
    "\n",
    "    return A, backprop_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_parameters(initialize, dimension_of_layers):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    initialization -- activation used in this layer. \n",
    "        Stored as text string: \"He\", \"Xavier\", \"Yoshua\" \"random\"\n",
    "    dimensions_of_layers -- array (list) of size in each layer\n",
    "\n",
    "    Returns:\n",
    "    parameters -- dictionary containing parameters \"W1\", \"b1\", \"W2\", \"b2\",...\n",
    "                W[layer] -- shape (dimension_of_layers[layer], (dimension_of_layers[layer-1])\n",
    "                b[layer] -- bias vector shape (dimension_of_layers[layer], 1) \n",
    "    \"\"\"\n",
    "    \n",
    "    # np.random.seed(1)  # Use when you need to test that the different initializations are giving different numbers\n",
    "    parameters = {}\n",
    "    num_layers = len(dimension_of_layers)\n",
    "\n",
    "    for layer in range(1, num_layers):  # this will loop through first hidden layer to final output layer\n",
    "\n",
    "        if initialize == \"He\":\n",
    "            parameters[\"W\" + str(layer)] = np.random.randn(dimension_of_layers[layer], \n",
    "                dimension_of_layers[layer - 1]) * np.sqrt(2. / dimension_of_layers[layer - 1])\n",
    "            parameters[\"b\" + str(layer)] = np.zeros( (dimension_of_layers[layer], 1) )\n",
    "\n",
    "        elif initialize == \"Yoshua\":\n",
    "            parameters[\"W\" + str(layer)] = np.random.randn(dimension_of_layers[layer], \n",
    "                dimension_of_layers[layer - 1]) * np.sqrt(2. / (dimension_of_layers[layer - 1] + dimension_of_layers[layer]))\n",
    "            parameters[\"b\" + str(layer)] = np.zeros( (dimension_of_layers[layer], 1) )\n",
    "\n",
    "        elif initialize == \"Xavier\":\n",
    "            parameters[\"W\" + str(layer)] = np.random.randn(dimension_of_layers[layer], \n",
    "                dimension_of_layers[layer - 1]) * np.sqrt(1. / (dimension_of_layers[layer - 1]))\n",
    "            parameters[\"b\" + str(layer)] = np.zeros( (dimension_of_layers[layer], 1) )\n",
    "\n",
    "        elif initialize == \"random\":\n",
    "            parameters[\"W\" + str(layer)] = np.random.randn(dimension_of_layers[layer], dimension_of_layers[layer - 1]) * 0.01\n",
    "            parameters[\"b\" + str(layer)] = np.zeros( (dimension_of_layers[layer], 1) )\n",
    "\n",
    "        else:\n",
    "            print(\"ERROR: YOU MUST CHOOSE AN INITIALIZATION TYPE: \\\"He\\\", \\\"Yoshua\\\", \\\"Xavier\\\", or \\\"random\\\"\")\n",
    "\n",
    "            assert parameters[\"W\" + str(layer)].shape == (dimension_of_layers[layer], dimension_of_layers[layer - 1])\n",
    "            assert parameters[\"b\" + str(layer)].shape == (dimension_of_layers[layer], 1)\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_forward(A_prev, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    A_post -- the input of the activation function, also called pre-activation parameter \n",
    "    backprop_store -- a python dictionary containing \"W\", \"b\", and \"A_prev\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "\n",
    "    Z = np.dot(W, A_prev) + b\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    backprop_store = (W, b, A_prev)\n",
    "    \n",
    "    return Z, backprop_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, hidden_activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    hidden_activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\", \"relu\",\n",
    "                         \"leaky relu\", or \"tanh\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python dictionary containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "             \"linear_cache\" and \"activation_cache\" are caching, storing, exactly what's being passed in it's function.\n",
    "    \"\"\"\n",
    "    \n",
    "    if hidden_activation == \"sigmoid\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    \n",
    "    elif hidden_activation == \"relu\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "\n",
    "    elif hidden_activation == \"leaky relu\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = leaky_relu(Z)\n",
    "\n",
    "    elif hidden_activation == \"tanh\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = tanh(Z)\n",
    "\n",
    "    elif hidden_activation == \"softmax\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = softmax(Z)\n",
    "    \n",
    "    else:\n",
    "        print(\"ERROR: YOU MUST CHOOSE A HIDDEN AND OUTPUT ACTIVATION. HIDDEN TYPES: \\\"sigmoid\\\", \\\"relu\\\", \\\"leaky relu\\\", or \\\"tanh\\\". OUTPUT TYPES: \\\"sigmoid\\\" or \\\"softmax\\\"\")\n",
    "\n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters, dimension_of_layers, hidden_activation, output_activation):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "    dimension_of_layers -- array (list) of size in each layer\n",
    "    hidden_activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\", \"relu\",\n",
    "                         \"leaky relu\", or \"tanh\"\n",
    "    output_activation -- the activation to be used in the output layer (L), stored as a text string:\n",
    "                         \"sigmoid\" or \"softmax\"\n",
    "    \n",
    "    \n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() (there are L-1 of them, indexed from 0 to L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(dimension_of_layers)\n",
    "    \n",
    "    # when using range in the loop below it will end at the final hidden loop\n",
    "    # Calculating AL for the output layer also used L-1 because you did not use\n",
    "    # a loop with range. So, you need to put L-1 due to Python starting index from 0.\n",
    "    \n",
    "    # Implement [LINEAR -> hidden_activation]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for layer in range(1, L-1):    # This will loop through first hidden layer to last hidden layer (before output layer L)\n",
    "        A_prev = A \n",
    "        A, cache = linear_activation_forward(A_prev, parameters[\"W\" + str(layer)], parameters[\"b\" + str(layer)], hidden_activation)\n",
    "        caches.append(cache)\n",
    "    \n",
    "    # use the A (now the last hidden layer activation after the loop) to get activation of final the layer (AL)\n",
    "    # Implement LINEAR -> outut_activation. Add \"cache\" to the \"caches\" list.\n",
    "    AL, cache = linear_activation_forward(A, parameters[\"W\" + str(L-1)], parameters[\"b\" + str(L-1)], output_activation)\n",
    "    caches.append(cache)\n",
    "    \n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "            \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### skipped cost function for softmax. Add it in when you have more free time.\n",
    "\n",
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function, cross-entropy cost for \"sigmoid\" function\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector, shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # Compute loss from aL and y.\n",
    "    cost = cost = (   (    np.dot(Y, np.log(AL.T))   )  +  (np.dot(  (1 - Y), np.log(1 - AL.T)  )  )   )  /  -m\n",
    "    \n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = np.dot(dZ, A_prev.T) / m\n",
    "    db = np.sum(dZ, axis = 1, keepdims = True) / m       ## remember this one for that type of formula!\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### need to also make backward function for tanh and softmax\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single RELU unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    \n",
    "    # When z <= 0, you should set dz to 0 as well. \n",
    "    dZ[Z <= 0] = 0  # wherever Z is less than or equal to 0 then 0 \n",
    "                    # will be placed in dZ for that index.This works because they are\n",
    "                    # the same size\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "def sigmoid_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single SIGMOID unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    \n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "def leaky_relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single RELU unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    \n",
    "    # When z <= 0, you should set dz to 0 as well. \n",
    "    dZ[Z <= 0] = 0.01  # wherever Z is less than or equal to 0 then 0.01 \n",
    "                    # will be placed in dZ for that index.This works because they are\n",
    "                    # the same size\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## need to make one for softmax too, tanh, and leaky relu too\n",
    "\n",
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##change this so that you have 2 options for the initialize backprop. one for\n",
    "## sigmoid which is written and one for softmax which is dAL\n",
    "\n",
    "def L_model_backward(AL, Y, caches, dimension_of_layers):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(dimension_of_layers) # the number of layers #you counted the input as a layer  ####### WATCH FORWARD AND BACKWARD PROP VIDEO TO KINDA UNDERSTAND\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    \n",
    "    ## dAL for softmax here -> #you have to write the code#\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    \n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"dAL, current_cache\". Outputs: \"grads[\"dAL-2\"], grads[\"dWL -1\"], grads[\"dbL-1\"]\n",
    "    current_cache = caches[L - 2]    # this is accounting that Python indexes from 0 and that the first cache starts at layer 2\n",
    "    grads[\"dA\" + str(L-2)], grads[\"dW\" + str(L-1)], grads[\"db\" + str(L-1)] = linear_activation_backward(dAL, current_cache, \"sigmoid\")\n",
    "    \n",
    "    \n",
    "    ########################stopped here\n",
    "    # Loop from l=L-2 to l=0\n",
    "    for l in reversed(range(L-1)): # This is actually accounting Python indexes from 0 AND that range goes only to number before.\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        # Inputs: \"grads[\"dA\" + str(l + 1)], current_cache\". Outputs: \"grads[\"dA\" + str(l)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
    "        current_cache = caches[L - 2]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 1)], current_cache, \"relu\")\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.random.rand(3, 8)\n",
    "dimension_of_layers = (3, 4, 4, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters = initialize_parameters(\"He\", dimension_of_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "AL, cache = L_model_forward(X, parameters, dimension_of_layers, \"relu\", \"sigmoid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for l in reversed(range(4-2)):\n",
    "    print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-105-3ab48a95cf38>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m.5\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'float' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "m = 10\n",
    "p = .5\n",
    "y = [1,1,1,1,1,1,1,1,1,1]\n",
    "np.log(p[range(m),y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.41675785, -0.05626683, -2.1361961 ,  1.64027081],\n",
       "       [-1.79343559, -0.84174737,  0.50288142, -1.24528809]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(2)\n",
    "dimension_of_layers = [2, 3, 1]\n",
    "X = np.random.randn(2,4)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters = initialize_parameters(\"He\", dimension_of_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[-1.05795222, -0.90900761],\n",
       "        [ 0.55145404,  2.29220801],\n",
       "        [ 0.04153939, -1.11792545]]),\n",
       " 'W2': array([[ 0.44013928, -0.48676236, -0.01561999]]),\n",
       " 'b1': array([[ 0.],\n",
       "        [ 0.],\n",
       "        [ 0.]]),\n",
       " 'b2': array([[ 0.]])}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output, cache = L_model_forward(X, parameters, dimension_of_layers, \"relu\", \"softmax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((array([[-1.05795222, -0.90900761],\n",
       "          [ 0.55145404,  2.29220801],\n",
       "          [ 0.04153939, -1.11792545]]), array([[ 0.],\n",
       "          [ 0.],\n",
       "          [ 0.]]), array([[-0.41675785, -0.05626683, -2.1361961 ,  1.64027081],\n",
       "          [-1.79343559, -0.84174737,  0.50288142, -1.24528809]])),\n",
       "  array([[ 2.07115649,  0.82468238,  1.80287036, -0.60335179],\n",
       "         [-4.34075022, -1.96048863, -0.02530516, -1.94992536],\n",
       "         [ 1.98761541,  0.93867351, -0.65092022,  1.46027509]])),\n",
       " ((array([[ 0.44013928, -0.48676236, -0.01561999]]),\n",
       "   array([[ 0.]]),\n",
       "   array([[ 2.07115649,  0.82468238,  1.80287036,  0.        ],\n",
       "          [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "          [ 1.98761541,  0.93867351,  0.        ,  1.46027509]])),\n",
       "  array([[ 0.8805508 ,  0.34831304,  0.79351406, -0.02280948]]))]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = np.array([[0,0,0], [0,1,1],[1,0,0]])\n",
    "y = ([[2,1,1]])\n",
    "p = np.array([[.1,.9, .1], [.1, .45, .6], [.8, .45, .3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.1   0.9   0.1 ]\n",
      " [ 0.1   0.45  0.6 ]\n",
      " [ 0.8   0.45  0.3 ]]\n",
      "[[0 0 0]\n",
      " [0 1 1]\n",
      " [1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(p)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4973411560058594"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tic = time.time()\n",
    "p[y, range(3)]\n",
    "toc= time.time()\n",
    "(toc - tic)*1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4954338073730469"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tic = time.time()\n",
    "a*p\n",
    "toc=time.time()\n",
    "(toc - tic)*1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = np.random.randn(3, 5000)\n",
    "y = np.random.randint(2,5000)\n",
    "a = np.random.randn(3,5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1009 is out of bounds for axis 0 with size 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-b4cb57c4dbeb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtic\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtoc\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtoc\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 1009 is out of bounds for axis 0 with size 3"
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "p[y, range(5000)]\n",
    "toc= time.time()\n",
    "(toc - tic)*1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
